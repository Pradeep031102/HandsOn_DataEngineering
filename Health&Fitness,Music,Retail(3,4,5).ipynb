{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr5lW3gK3NXN",
        "outputId": "ee403f44-2d8b-4af6-f006-45f7483303ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO5MA65g5VgT",
        "outputId": "e6ab54c5-f206-4f91-c515-92abdcb2c6a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|      1|2023-09-01|12000|            500|           7.0|      Cardio|\n",
            "|      2|2023-09-01| 8000|            400|           6.5|    Strength|\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|\n",
            "|      1|2023-09-02|10000|            450|           6.0|      Cardio|\n",
            "|      2|2023-09-02| 9500|            500|           7.0|      Cardio|\n",
            "|      3|2023-09-02|14000|            600|           7.5|    Strength|\n",
            "|      1|2023-09-03|13000|            550|           8.0|        Yoga|\n",
            "|      2|2023-09-03|12000|            520|           6.5|        Yoga|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, rank\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType\n",
        "\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Health & Fitness Tracker\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", LongType(), True),\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"steps\", LongType(), True),\n",
        "    StructField(\"calories_burned\", LongType(), True),  \n",
        "    StructField(\"hours_of_sleep\", DoubleType(), True),\n",
        "    StructField(\"workout_type\", StringType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "fitness_data = [\n",
        "    (1, '2023-09-01', 12000, 500, 7.0, 'Cardio'),\n",
        "    (2, '2023-09-01', 8000, 400, 6.5, 'Strength'),\n",
        "    (3, '2023-09-01', 15000, 650, 8.0, 'Yoga'),\n",
        "    (1, '2023-09-02', 10000, 450, 6.0, 'Cardio'),\n",
        "    (2, '2023-09-02', 9500, 500, 7.0, 'Cardio'),\n",
        "    (3, '2023-09-02', 14000, 600, 7.5, 'Strength'),\n",
        "    (1, '2023-09-03', 13000, 550, 8.0, 'Yoga'),\n",
        "    (2, '2023-09-03', 12000, 520, 6.5, 'Yoga'),\n",
        "    (3, '2023-09-03', 16000, 700, 7.0, 'Cardio')\n",
        "]\n",
        "\n",
        "\n",
        "fitness_df = spark.createDataFrame(fitness_data, schema)\n",
        "\n",
        "fitness_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY2DMxE7596-",
        "outputId": "fe55f564-db49-4161-b9f7-741c6eb927ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+\n",
            "|user_id|total_steps|\n",
            "+-------+-----------+\n",
            "|      1|      35000|\n",
            "|      3|      45000|\n",
            "|      2|      29500|\n",
            "+-------+-----------+\n",
            "\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|      1|2023-09-01|12000|            500|           7.0|      Cardio|\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|\n",
            "|      3|2023-09-02|14000|            600|           7.5|    Strength|\n",
            "|      1|2023-09-03|13000|            550|           8.0|        Yoga|\n",
            "|      2|2023-09-03|12000|            520|           6.5|        Yoga|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "\n",
            "+------------+-------------------+\n",
            "|workout_type|avg_calories_burned|\n",
            "+------------+-------------------+\n",
            "|    Strength|              500.0|\n",
            "|        Yoga|  573.3333333333334|\n",
            "|      Cardio|              537.5|\n",
            "+------------+-------------------+\n",
            "\n",
            "+-------+----------+-----------+----+\n",
            "|user_id|      date|total_steps|rank|\n",
            "+-------+----------+-----------+----+\n",
            "|      1|2023-09-03|      13000|   1|\n",
            "|      2|2023-09-03|      12000|   1|\n",
            "|      3|2023-09-03|      16000|   1|\n",
            "+-------+----------+-----------+----+\n",
            "\n",
            "+-------+\n",
            "|user_id|\n",
            "+-------+\n",
            "|      3|\n",
            "+-------+\n",
            "\n",
            "+-------+------------------+\n",
            "|user_id|avg_hours_of_sleep|\n",
            "+-------+------------------+\n",
            "|      1|               7.0|\n",
            "|      3|               7.5|\n",
            "|      2| 6.666666666666667|\n",
            "+-------+------------------+\n",
            "\n",
            "+----------+---------------------+\n",
            "|      date|total_calories_burned|\n",
            "+----------+---------------------+\n",
            "|2023-09-01|                 1550|\n",
            "|2023-09-02|                 1550|\n",
            "|2023-09-03|                 1770|\n",
            "+----------+---------------------+\n",
            "\n",
            "+-------+-------------------+\n",
            "|user_id|workout_types_count|\n",
            "+-------+-------------------+\n",
            "|      1|                  2|\n",
            "|      3|                  3|\n",
            "|      2|                  3|\n",
            "+-------+-------------------+\n",
            "\n",
            "+-------+--------------+\n",
            "|user_id|total_workouts|\n",
            "+-------+--------------+\n",
            "|      1|             3|\n",
            "|      3|             3|\n",
            "|      2|             3|\n",
            "+-------+--------------+\n",
            "\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|active_day|\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "|      1|2023-09-01|12000|            500|           7.0|      Cardio|    Active|\n",
            "|      2|2023-09-01| 8000|            400|           6.5|    Strength|  Inactive|\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|    Active|\n",
            "|      1|2023-09-02|10000|            450|           6.0|      Cardio|  Inactive|\n",
            "|      2|2023-09-02| 9500|            500|           7.0|      Cardio|  Inactive|\n",
            "|      3|2023-09-02|14000|            600|           7.5|    Strength|    Active|\n",
            "|      1|2023-09-03|13000|            550|           8.0|        Yoga|    Active|\n",
            "|      2|2023-09-03|12000|            520|           6.5|        Yoga|    Active|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|    Active|\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Find the Total Steps Taken by Each User\n",
        "total_steps_per_user = fitness_df.groupBy(\"user_id\").agg(sum(\"steps\").alias(\"total_steps\"))\n",
        "total_steps_per_user.show()\n",
        "\n",
        "# 2. Filter Days with More Than 10,000 Steps\n",
        "days_gt_10000_steps = fitness_df.filter(col(\"steps\") > 10000)\n",
        "days_gt_10000_steps.show()\n",
        "\n",
        "# 3. Calculate the Average Calories Burned by Workout Type\n",
        "avg_calories_by_workout = fitness_df.groupBy(\"workout_type\").agg(avg(\"calories_burned\").alias(\"avg_calories_burned\"))\n",
        "avg_calories_by_workout.show()\n",
        "\n",
        "# 4. Identify the Day with the Most Steps for Each User\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"total_steps\").desc())\n",
        "most_steps_per_day = fitness_df.groupBy(\"user_id\", \"date\").agg(sum(\"steps\").alias(\"total_steps\")) \\\n",
        "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
        "    .filter(col(\"rank\") == 1)\n",
        "\n",
        "# Show the result\n",
        "most_steps_per_day.show()\n",
        "# 5. Find Users Who Burned More Than 600 Calories on Any Day\n",
        "users_burned_600_calories = fitness_df.filter(col(\"calories_burned\") > 600) \\\n",
        "    .select(\"user_id\").distinct()\n",
        "users_burned_600_calories.show()\n",
        "\n",
        "# 6. Calculate the Average Hours of Sleep per User\n",
        "avg_sleep_per_user = fitness_df.groupBy(\"user_id\").agg(avg(\"hours_of_sleep\").alias(\"avg_hours_of_sleep\"))\n",
        "avg_sleep_per_user.show()\n",
        "\n",
        "# 7. Find the Total Calories Burned per Day\n",
        "total_calories_per_day = fitness_df.groupBy(\"date\").agg(sum(\"calories_burned\").alias(\"total_calories_burned\"))\n",
        "total_calories_per_day.show()\n",
        "\n",
        "# 8. Identify Users Who Did Different Types of Workouts\n",
        "users_multiple_workouts = fitness_df.groupBy(\"user_id\").agg(countDistinct(\"workout_type\").alias(\"workout_types_count\")) \\\n",
        "    .filter(col(\"workout_types_count\") > 1)\n",
        "users_multiple_workouts.show()\n",
        "\n",
        "# 9. Calculate the Total Number of Workouts per User\n",
        "total_workouts_per_user = fitness_df.groupBy(\"user_id\").agg(count(\"workout_type\").alias(\"total_workouts\"))\n",
        "total_workouts_per_user.show()\n",
        "\n",
        "# 10. Create a New Column for \"Active\" Days\n",
        "df_with_activity = fitness_df.withColumn(\"active_day\", when(col(\"steps\") > 10000, \"Active\").otherwise(\"Inactive\"))\n",
        "df_with_activity.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M82S1lYY7Ark",
        "outputId": "27965996-9401-4520-acc7-7205fae4742d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|\n",
            "|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|\n",
            "|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Music Streaming\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "music_data = [\n",
        "    (1, 'Blinding Lights', 'The Weeknd', 200, '2023-09-01 08:15:00', 'New York'),\n",
        "    (2, 'Shape of You', 'Ed Sheeran', 240, '2023-09-01 09:20:00', 'Los Angeles'),\n",
        "    (3, 'Levitating', 'Dua Lipa', 180, '2023-09-01 10:30:00', 'London'),\n",
        "    (1, 'Starboy', 'The Weeknd', 220, '2023-09-01 11:00:00', 'New York'),\n",
        "    (2, 'Perfect', 'Ed Sheeran', 250, '2023-09-01 12:15:00', 'Los Angeles'),\n",
        "    (3, 'Don\\'t Start Now', 'Dua Lipa', 200, '2023-09-02 08:10:00', 'London'),\n",
        "    (1, 'Save Your Tears', 'The Weeknd', 210, '2023-09-02 09:00:00', 'New York'),\n",
        "    (2, 'Galway Girl', 'Ed Sheeran', 190, '2023-09-02 10:00:00', 'Los Angeles'),\n",
        "    (3, 'New Rules', 'Dua Lipa', 230, '2023-09-02 11:00:00', 'London')\n",
        "]\n",
        "music_columns = [\"user_id\", \"song_title\", \"artist\", \"duration_seconds\", \"streaming_time\", \"location\"]\n",
        "music_df = spark.createDataFrame(music_data, music_columns)\n",
        "music_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgKw5l5G7dwy",
        "outputId": "1b686637-ffcf-4ae7-d56d-a4938b91b5e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+\n",
            "|user_id|total_listening_time|\n",
            "+-------+--------------------+\n",
            "|      1|                 630|\n",
            "|      3|                 610|\n",
            "|      2|                 680|\n",
            "+-------+--------------------+\n",
            "\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "\n",
            "Most Popular Artist: Row(artist='Dua Lipa', total_streams=3)\n",
            "Longest Song: Row(user_id=2, song_title='Perfect', artist='Ed Sheeran', duration_seconds=250, streaming_time='2023-09-01 12:15:00', location='Los Angeles')\n",
            "+----------+--------------------+\n",
            "|    artist|avg_duration_seconds|\n",
            "+----------+--------------------+\n",
            "|  Dua Lipa|  203.33333333333334|\n",
            "|Ed Sheeran|  226.66666666666666|\n",
            "|The Weeknd|               210.0|\n",
            "+----------+--------------------+\n",
            "\n",
            "+-------+---------------+------------+----+\n",
            "|user_id|     song_title|stream_count|rank|\n",
            "+-------+---------------+------------+----+\n",
            "|      1|Blinding Lights|           1|   1|\n",
            "|      1|        Starboy|           1|   1|\n",
            "|      1|Save Your Tears|           1|   1|\n",
            "|      2|   Shape of You|           1|   1|\n",
            "|      2|    Galway Girl|           1|   1|\n",
            "|      2|        Perfect|           1|   1|\n",
            "|      3|     Levitating|           1|   1|\n",
            "|      3|      New Rules|           1|   1|\n",
            "|      3|Don't Start Now|           1|   1|\n",
            "+-------+---------------+------------+----+\n",
            "\n",
            "+----------+-------------+\n",
            "|      date|total_streams|\n",
            "+----------+-------------+\n",
            "|2023-09-01|            5|\n",
            "|2023-09-02|            4|\n",
            "+----------+-------------+\n",
            "\n",
            "+-------+------------+\n",
            "|user_id|artist_count|\n",
            "+-------+------------+\n",
            "+-------+------------+\n",
            "\n",
            "+-----------+-------------+\n",
            "|   location|total_streams|\n",
            "+-----------+-------------+\n",
            "|Los Angeles|            3|\n",
            "|     London|            3|\n",
            "|   New York|            3|\n",
            "+-----------+-------------+\n",
            "\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|song_length|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|      Short|\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|       Long|\n",
            "|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|      Short|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|       Long|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|       Long|\n",
            "|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|      Short|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|       Long|\n",
            "|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|      Short|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|       Long|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Calculate the Total Listening Time for Each User\n",
        "total_listening_time_per_user = music_df.groupBy(\"user_id\").agg(sum(\"duration_seconds\").alias(\"total_listening_time\"))\n",
        "total_listening_time_per_user.show()\n",
        "\n",
        "# 2. Filter Songs Streamed for More Than 200 Seconds\n",
        "songs_gt_200_seconds = music_df.filter(col(\"duration_seconds\") > 200)\n",
        "songs_gt_200_seconds.show()\n",
        "\n",
        "# 3. Find the Most Popular Artist (by Total Streams)\n",
        "most_popular_artist = music_df.groupBy(\"artist\").agg(count(\"song_title\").alias(\"total_streams\")) \\\n",
        "    .orderBy(col(\"total_streams\").desc()).first()\n",
        "print(f\"Most Popular Artist: {most_popular_artist}\")\n",
        "\n",
        "# 4. Identify the Song with the Longest Duration\n",
        "longest_song = music_df.orderBy(col(\"duration_seconds\").desc()).first()\n",
        "print(f\"Longest Song: {longest_song}\")\n",
        "\n",
        "# 5. Calculate the Average Song Duration by Artist\n",
        "avg_duration_by_artist = music_df.groupBy(\"artist\").agg(avg(\"duration_seconds\").alias(\"avg_duration_seconds\"))\n",
        "avg_duration_by_artist.show()\n",
        "\n",
        "# 6. Find the Top 3 Most Streamed Songs per User\n",
        "top_songs_per_user = music_df.groupBy(\"user_id\", \"song_title\") \\\n",
        "    .agg(count(\"song_title\").alias(\"stream_count\")) \\\n",
        "    .withColumn(\"rank\", rank().over(Window.partitionBy(\"user_id\").orderBy(col(\"stream_count\").desc()))) \\\n",
        "    .filter(col(\"rank\") <= 3)\n",
        "top_songs_per_user.show()\n",
        "\n",
        "# 7. Calculate the Total Number of Streams per Day\n",
        "streams_per_day = music_df.groupBy(music_df.streaming_time.substr(1, 10).alias(\"date\")) \\\n",
        "    .agg(count(\"song_title\").alias(\"total_streams\"))\n",
        "streams_per_day.show()\n",
        "\n",
        "# 8. Identify Users Who Streamed Songs from More Than One Artist\n",
        "users_multiple_artists = music_df.groupBy(\"user_id\").agg(countDistinct(\"artist\").alias(\"artist_count\")) \\\n",
        "    .filter(col(\"artist_count\") > 1)\n",
        "users_multiple_artists.show()\n",
        "\n",
        "# 9. Calculate the Total Streams for Each Location\n",
        "streams_per_location = music_df.groupBy(\"location\").agg(count(\"song_title\").alias(\"total_streams\"))\n",
        "streams_per_location.show()\n",
        "\n",
        "# 10. Create a New Column to Classify Long and Short Songs\n",
        "df_with_song_length = music_df.withColumn(\"song_length\", when(col(\"duration_seconds\") > 200, \"Long\").otherwise(\"Short\"))\n",
        "df_with_song_length.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncOgPPyN7ppV",
        "outputId": "2c018e8b-2630-448e-a89d-e404fdcda2b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+------------+-----------+-----+--------+----------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|\n",
            "+--------------+------------+-----------+-----+--------+----------+\n",
            "|             1|       Apple|  Groceries|  0.5|      10|2023-09-01|\n",
            "|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|\n",
            "|             3|    Notebook| Stationery|  2.0|       5|2023-09-02|\n",
            "|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|\n",
            "|             6|       Pants|   Clothing| 25.0|       3|2023-09-03|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|\n",
            "|             8|         Pen| Stationery|  1.0|      10|2023-09-04|\n",
            "|             9|      Orange|  Groceries|  0.6|       8|2023-09-05|\n",
            "|            10|    Sneakers|   Clothing| 50.0|       1|2023-09-05|\n",
            "+--------------+------------+-----------+-----+--------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Retail Store Sales\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "retail_data = [\n",
        "    (1, 'Apple', 'Groceries', 0.50, 10, '2023-09-01'),\n",
        "    (2, 'T-shirt', 'Clothing', 15.00, 2, '2023-09-01'),\n",
        "    (3, 'Notebook', 'Stationery', 2.00, 5, '2023-09-02'),\n",
        "    (4, 'Banana', 'Groceries', 0.30, 12, '2023-09-02'),\n",
        "    (5, 'Laptop', 'Electronics', 800.00, 1, '2023-09-03'),\n",
        "    (6, 'Pants', 'Clothing', 25.00, 3, '2023-09-03'),\n",
        "    (7, 'Headphones', 'Electronics', 100.00, 2, '2023-09-04'),\n",
        "    (8, 'Pen', 'Stationery', 1.00, 10, '2023-09-04'),\n",
        "    (9, 'Orange', 'Groceries', 0.60, 8, '2023-09-05'),\n",
        "    (10, 'Sneakers', 'Clothing', 50.00, 1, '2023-09-05')\n",
        "]\n",
        "retail_columns = [\"transaction_id\", \"product_name\", \"category\", \"price\", \"quantity\", \"sales_date\"]\n",
        "retail_df = spark.createDataFrame(retail_data, retail_columns)\n",
        "retail_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_ngM-Lw71r0",
        "outputId": "2b557ddb-7b03-4051-e7b4-2c8e62191adb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------------+\n",
            "|   category|     total_revenue|\n",
            "+-----------+------------------+\n",
            "| Stationery|              20.0|\n",
            "|  Groceries|13.399999999999999|\n",
            "|Electronics|            1000.0|\n",
            "|   Clothing|             155.0|\n",
            "+-----------+------------------+\n",
            "\n",
            "+--------------+------------+-----------+-----+--------+----------+-----------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|total_sales|\n",
            "+--------------+------------+-----------+-----+--------+----------+-----------+\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|      800.0|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|      200.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+-----------+\n",
            "\n",
            "Most Sold Product: Row(product_name='Banana', total_quantity=12)\n",
            "+-----------+------------------+\n",
            "|   category|         avg_price|\n",
            "+-----------+------------------+\n",
            "| Stationery|               1.5|\n",
            "|  Groceries|0.4666666666666666|\n",
            "|Electronics|             450.0|\n",
            "|   Clothing|              30.0|\n",
            "+-----------+------------------+\n",
            "\n",
            "+------------+-------------+\n",
            "|product_name|total_revenue|\n",
            "+------------+-------------+\n",
            "|      Laptop|        800.0|\n",
            "|  Headphones|        200.0|\n",
            "|       Pants|         75.0|\n",
            "+------------+-------------+\n",
            "\n",
            "+----------+----------------+\n",
            "|sales_date|total_items_sold|\n",
            "+----------+----------------+\n",
            "|2023-09-01|              12|\n",
            "|2023-09-02|              17|\n",
            "|2023-09-03|               4|\n",
            "|2023-09-05|               9|\n",
            "|2023-09-04|              12|\n",
            "+----------+----------------+\n",
            "\n",
            "+--------------+------------+-----------+-----+--------+----------+------------------+-----------+------------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|       total_sales|   category|lowest_price|\n",
            "+--------------+------------+-----------+-----+--------+----------+------------------+-----------+------------+\n",
            "|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|              30.0|   Clothing|        15.0|\n",
            "|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|3.5999999999999996|  Groceries|         0.3|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|             200.0|Electronics|       100.0|\n",
            "|             8|         Pen| Stationery|  1.0|      10|2023-09-04|              10.0| Stationery|         1.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+------------------+-----------+------------+\n",
            "\n",
            "+------------+------------------+\n",
            "|product_name|     total_revenue|\n",
            "+------------+------------------+\n",
            "|     T-shirt|              30.0|\n",
            "|      Banana|3.5999999999999996|\n",
            "|      Laptop|             800.0|\n",
            "|    Notebook|              10.0|\n",
            "|       Apple|               5.0|\n",
            "|    Sneakers|              50.0|\n",
            "|      Orange|               4.8|\n",
            "|         Pen|              10.0|\n",
            "|       Pants|              75.0|\n",
            "|  Headphones|             200.0|\n",
            "+------------+------------------+\n",
            "\n",
            "+----------+-----------+------------------+\n",
            "|sales_date|   category|       total_sales|\n",
            "+----------+-----------+------------------+\n",
            "|2023-09-01|  Groceries|               5.0|\n",
            "|2023-09-02|  Groceries|3.5999999999999996|\n",
            "|2023-09-01|   Clothing|              30.0|\n",
            "|2023-09-02| Stationery|              10.0|\n",
            "|2023-09-03|Electronics|             800.0|\n",
            "|2023-09-05|  Groceries|               4.8|\n",
            "|2023-09-04| Stationery|              10.0|\n",
            "|2023-09-04|Electronics|             200.0|\n",
            "|2023-09-03|   Clothing|              75.0|\n",
            "|2023-09-05|   Clothing|              50.0|\n",
            "+----------+-----------+------------------+\n",
            "\n",
            "+--------------+------------+-----------+-----+--------+----------+------------------+----------------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|       total_sales|discounted_price|\n",
            "+--------------+------------+-----------+-----+--------+----------+------------------+----------------+\n",
            "|             1|       Apple|  Groceries|  0.5|      10|2023-09-01|               5.0|            0.45|\n",
            "|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|              30.0|            13.5|\n",
            "|             3|    Notebook| Stationery|  2.0|       5|2023-09-02|              10.0|             1.8|\n",
            "|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|3.5999999999999996|            0.27|\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|             800.0|           720.0|\n",
            "|             6|       Pants|   Clothing| 25.0|       3|2023-09-03|              75.0|            22.5|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|             200.0|            90.0|\n",
            "|             8|         Pen| Stationery|  1.0|      10|2023-09-04|              10.0|             0.9|\n",
            "|             9|      Orange|  Groceries|  0.6|       8|2023-09-05|               4.8|            0.54|\n",
            "|            10|    Sneakers|   Clothing| 50.0|       1|2023-09-05|              50.0|            45.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Calculate the Total Revenue per Category\n",
        "retail_df = retail_df.withColumn(\"total_sales\", col(\"price\") * col(\"quantity\"))\n",
        "total_revenue_per_category = retail_df.groupBy(\"category\").agg(sum(\"total_sales\").alias(\"total_revenue\"))\n",
        "total_revenue_per_category.show()\n",
        "\n",
        "# 2. Filter Transactions Where the Total Sales Amount is Greater Than $100\n",
        "transactions_gt_100 = retail_df.filter(col(\"total_sales\") > 100)\n",
        "transactions_gt_100.show()\n",
        "\n",
        "# 3. Find the Most Sold Product\n",
        "most_sold_product = retail_df.groupBy(\"product_name\").agg(sum(\"quantity\").alias(\"total_quantity\")) \\\n",
        "    .orderBy(col(\"total_quantity\").desc()).first()\n",
        "print(f\"Most Sold Product: {most_sold_product}\")\n",
        "\n",
        "# 4. Calculate the Average Price per Product Category\n",
        "avg_price_per_category = retail_df.groupBy(\"category\").agg(avg(\"price\").alias(\"avg_price\"))\n",
        "avg_price_per_category.show()\n",
        "\n",
        "# 5. Find the Top 3 Highest Grossing Products\n",
        "top_grossing_products = retail_df.groupBy(\"product_name\").agg(sum(\"total_sales\").alias(\"total_revenue\")) \\\n",
        "    .orderBy(col(\"total_revenue\").desc()).limit(3)\n",
        "top_grossing_products.show()\n",
        "\n",
        "# 6. Calculate the Total Number of Items Sold per Day\n",
        "total_items_sold_per_day = retail_df.groupBy(\"sales_date\").agg(sum(\"quantity\").alias(\"total_items_sold\"))\n",
        "total_items_sold_per_day.show()\n",
        "\n",
        "# 7. Identify the Product with the Lowest Price in Each Category\n",
        "lowest_price_per_category = retail_df.groupBy(\"category\").agg(min(\"price\").alias(\"lowest_price\"))\n",
        "\n",
        "# Join the original DataFrame with the DataFrame containing lowest prices\n",
        "lowest_price_per_product = retail_df.join(\n",
        "    lowest_price_per_category,\n",
        "    on=(retail_df[\"category\"] == lowest_price_per_category[\"category\"]) & (retail_df[\"price\"] == lowest_price_per_category[\"lowest_price\"])\n",
        ")\n",
        "\n",
        "lowest_price_per_product.show()\n",
        "\n",
        "# 8. Calculate the Total Revenue for Each Product\n",
        "total_revenue_per_product = retail_df.groupBy(\"product_name\").agg(sum(\"total_sales\").alias(\"total_revenue\"))\n",
        "total_revenue_per_product.show()\n",
        "\n",
        "# 9. Find the Total Sales per Day for Each Category\n",
        "sales_per_day_per_category = retail_df.groupBy(\"sales_date\", \"category\").agg(sum(\"total_sales\").alias(\"total_sales\"))\n",
        "sales_per_day_per_category.show()\n",
        "\n",
        "# 10. Create a New Column for Discounted Price\n",
        "df_with_discount = retail_df.withColumn(\"discounted_price\", col(\"price\") * 0.9)\n",
        "df_with_discount.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
