{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "04bc7df3-1a1e-4c3d-a296-a1f45c1a7c85",
          "showTitle": false,
          "title": ""
        },
        "id": "X0cIQ5Kms0x0",
        "outputId": "22aa63a5-a61d-497f-c31c-f22a56dcb4f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+--------+--------+-----+\n",
            "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
            "+-------+----------+----------+--------+--------+-----+\n",
            "|   1001|2024-01-15|      C001|Widget A|      10| 25.5|\n",
            "|   1002|2024-01-16|      C002|Widget B|       5|15.75|\n",
            "|   1003|2024-01-16|      C001|Widget C|       8| 22.5|\n",
            "|   1004|2024-01-17|      C003|Widget A|      15| 25.5|\n",
            "|   1005|2024-01-18|      C004|Widget D|       7| 30.0|\n",
            "|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n",
            "|   1007|2024-01-20|      C005|Widget C|      12| 22.5|\n",
            "|   1008|2024-01-21|      C003|Widget A|      10| 25.5|\n",
            "+-------+----------+----------+--------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1\n",
        "dbutils.fs.cp(\"file:/Workspace/Shared/sales_data.csv\", \"dbfs:/FileStore/sales_data.csv\")\n",
        "dbutils.fs.cp(\"file:/Workspace/Shared/customer_data.json\", \"dbfs:/FileStore/customer_data.json\")\n",
        "dbutils.fs.cp(\"file:/Workspace/Shared/new_sales_data.csv\", \"dbfs:/FileStore/new_sales_data.csv\")\n",
        "\n",
        "# 1. Load the sales_data.csv file into a DataFrame.\n",
        "sales_df = spark.read.csv(\"dbfs:/FileStore/sales_data.csv\", header=True, inferSchema=True)\n",
        "sales_df.show()\n",
        "\n",
        "# 2. Write the DataFrame as a Delta Table.\n",
        "sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/sales_delta_table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1d773e14-1294-49fe-b97e-9162dad217a5",
          "showTitle": false,
          "title": ""
        },
        "id": "ukJZZqcKs0x1",
        "outputId": "78b2cfd8-e15f-4e83-82a6-d5bde315f964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------+------+----------+\n",
            "|CustomerID| CustomerName|Region|SignupDate|\n",
            "+----------+-------------+------+----------+\n",
            "|      C001|     John Doe| North|2022-07-01|\n",
            "|      C002|   Jane Smith| South|2023-02-15|\n",
            "|      C003|Emily Johnson|  East|2021-11-20|\n",
            "|      C004|Michael Brown|  West|2022-12-05|\n",
            "|      C005|  Linda Davis| North|2023-03-10|\n",
            "+----------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. Load the customer_data.json file into a DataFrame.\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
        "\n",
        "customer_schema = StructType([\n",
        "    StructField(\"CustomerID\", StringType(), True),\n",
        "    StructField(\"CustomerName\", StringType(), True),\n",
        "    StructField(\"Region\", StringType(), True),\n",
        "    StructField(\"SignupDate\", DateType(), True)\n",
        "])\n",
        "\n",
        "customer_df = spark.read.format(\"json\") \\\n",
        "    .schema(customer_schema) \\\n",
        "    .load(\"dbfs:/FileStore/customer_data.json\")\n",
        "\n",
        "customer_df.show()\n",
        "\n",
        "# 4. Write the DataFrame as a Delta Table.\n",
        "customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/customer_delta_table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dcb0d786-7433-43da-bdc3-7a070e1d9421",
          "showTitle": false,
          "title": ""
        },
        "id": "_IIqXXIws0x2"
      },
      "outputs": [],
      "source": [
        "# 5. Convert an existing Parquet file into a Delta Table (For demonstration, use a Parquet file available in your workspace).\n",
        "\n",
        "customer_df = spark.read.format(\"parquet\") \\\n",
        "    .load(\"dbfs:/FileStore/customer_data.parquet\")\n",
        "\n",
        "customer_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"dbfs:/FileStore/delta/customer_delta_table\")\n",
        "\n",
        "sales_df = spark.read.format(\"parquet\") \\\n",
        "    .load(\"dbfs:/FileStore/sales_data.parquet\")\n",
        "\n",
        "sales_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"dbfs:/FileStore/delta/sales_delta_table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ee01fb56-888d-4662-9ba2-2fbed773fe40",
          "showTitle": false,
          "title": ""
        },
        "id": "2nylba7Ds0x2",
        "outputId": "b8cb9ff2-31c0-43a9-8d0a-30d1d654d7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+--------+--------+-----+\n",
            "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
            "+-------+----------+----------+--------+--------+-----+\n",
            "|   1009|2024-01-22|      C006|Widget E|      14| 20.0|\n",
            "|   1010|2024-01-23|      C007|Widget F|       6| 35.0|\n",
            "|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n",
            "+-------+----------+----------+--------+--------+-----+\n",
            "\n",
            "+-------+----------+----------+--------+--------+-----+\n",
            "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
            "+-------+----------+----------+--------+--------+-----+\n",
            "|   1009|2024-01-22|      C006|Widget E|      14| 20.0|\n",
            "|   1010|2024-01-23|      C007|Widget F|       6| 35.0|\n",
            "|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n",
            "+-------+----------+----------+--------+--------+-----+\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Data Management\n",
        "# 1. Load the new_sales_data.csv file into a DataFrame.\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"OrderID\", IntegerType(), True),\n",
        "    StructField(\"OrderDate\", StringType(), True),\n",
        "    StructField(\"CustomerID\", StringType(), True),\n",
        "    StructField(\"Product\", StringType(), True),\n",
        "    StructField(\"Quantity\", IntegerType(), True),\n",
        "    StructField(\"Price\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "new_sales_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(schema) \\\n",
        "    .load(\"dbfs:/FileStore/new_sales_data.csv\")\n",
        "\n",
        "new_sales_df.show()\n",
        "\n",
        "# 2. Write the new DataFrame as a Delta Table.\n",
        "new_sales_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .save(\"dbfs:/FileStore/delta/new_sales_delta_table\")\n",
        "\n",
        "new_sales_df.show()\n",
        "\n",
        "\n",
        "# 3. Perform a MERGE INTO operation to update and insert records into the existing Delta table.\n",
        "spark.sql(\"\"\"\n",
        "MERGE INTO delta.`dbfs:/FileStore/delta/sales_delta_table` AS target\n",
        "USING delta.`dbfs:/FileStore/delta/new_sales_delta_table` AS source\n",
        "ON target.OrderID = source.OrderID\n",
        "WHEN MATCHED THEN\n",
        "  UPDATE SET\n",
        "    target.OrderDate = source.OrderDate,\n",
        "    target.CustomerID = source.CustomerID,\n",
        "    target.Product = source.Product,\n",
        "    target.Quantity = source.Quantity,\n",
        "    target.Price = source.Price\n",
        "WHEN NOT MATCHED THEN\n",
        "  INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price)\n",
        "  VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "549b2cd4-da72-42bb-9ac4-f32809c0bcde",
          "showTitle": false,
          "title": ""
        },
        "id": "dOToh8Was0x3",
        "outputId": "2274f6ea-c97a-49df-aa59-07a981cb6ac6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Optimize Delta Table\n",
        "# 1. Apply the OPTIMIZE command on the Delta Table and use Z-Ordering on an appropriate column.\n",
        "spark.sql(\"OPTIMIZE delta.`dbfs:/FileStore/delta/sales_delta_table`ZORDER BY (OrderDate)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e9115433-0bd4-4b89-941e-0fc6627544d6",
          "showTitle": false,
          "title": ""
        },
        "id": "3UtnzEzls0x3",
        "outputId": "194e79f6-f808-4359-90ec-4fc54c605ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------------+----------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
            "|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                                            |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |userMetadata|engineInfo                                |\n",
            "+-------+-------------------+----------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
            "|2      |2024-09-13 06:50:38|6016586593975937|azuser2134_mml.local@techademy.com|OPTIMIZE |{predicate -> [], zOrderBy -> [], batchId -> 0, auto -> true}                                                                                                                                                  |NULL|{1277694103907368}|0912-035714-9put9a6m|1          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 3092, p25FileSize -> 1714, numDeletionVectorsRemoved -> 1, minFileSize -> 1714, numAddedFiles -> 1, maxFileSize -> 1714, p75FileSize -> 1714, p50FileSize -> 1714, numAddedBytes -> 1714}                                                                                                                                                                                                                                                                                                                                                                                                                 |NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n",
            "|1      |2024-09-13 06:50:36|6016586593975937|azuser2134_mml.local@techademy.com|MERGE    |{predicate -> [\"(OrderID#10978 = OrderID#10990)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{1277694103907368}|0912-035714-9put9a6m|0          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1444, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 1685, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 637, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1023}|NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n",
            "|0      |2024-09-13 06:43:18|6016586593975937|azuser2134_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                   |NULL|{1277694103907368}|0912-035714-9put9a6m|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1648}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n",
            "+-------+-------------------+----------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[path: string]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4.Advanced Features\n",
        "# 1. Use DESCRIBE HISTORY to inspect the history of changes for a Delta Table.\n",
        "history_df = spark.sql(\"DESCRIBE HISTORY delta.`dbfs:/FileStore/delta/sales_delta_table`\")\n",
        "history_df.show(truncate=False)\n",
        "\n",
        "# 2. Use VACUUM to remove old files from the Delta Table.\n",
        "spark.sql(\"VACUUM delta.`dbfs:/FileStore/delta/sales_delta_table` RETAIN 168 HOURS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f474a29e-a4d5-447c-89b3-a9844484237e",
          "showTitle": false,
          "title": ""
        },
        "id": "TVa4Cnv1s0x3",
        "outputId": "41aa13b9-ecf5-4892-b979-d534eab6b374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+--------+--------+-----+\n",
            "|OrderID|OrderDate |CustomerID|Product |Quantity|Price|\n",
            "+-------+----------+----------+--------+--------+-----+\n",
            "|1001   |2024-01-15|C001      |Widget A|10      |25.5 |\n",
            "|1003   |2024-01-16|C001      |Widget C|8       |22.5 |\n",
            "|1004   |2024-01-17|C003      |Widget A|15      |25.5 |\n",
            "|1005   |2024-01-18|C004      |Widget D|7       |30.0 |\n",
            "|1006   |2024-01-19|C002      |Widget B|9       |15.75|\n",
            "|1007   |2024-01-20|C005      |Widget C|12      |22.5 |\n",
            "|1008   |2024-01-21|C003      |Widget A|10      |25.5 |\n",
            "|1002   |2024-01-16|C002      |Widget B|10      |15.75|\n",
            "|1009   |2024-01-22|C006      |Widget E|14      |20.0 |\n",
            "|1010   |2024-01-23|C007      |Widget F|6       |35.0 |\n",
            "+-------+----------+----------+--------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. (Handson Exercises)\n",
        "# 1. Using Delta Lake for Data Versioning: Query historical versions of the Delta Table using Time Travel.\n",
        "version_number = 2\n",
        "historical_df = spark.sql(f\"SELECT * FROM delta.`dbfs:/FileStore/delta/sales_delta_table` VERSION AS OF {version_number}\")\n",
        "historical_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c16cf117-e569-4ed3-9c27-4a8547629430",
          "showTitle": false,
          "title": ""
        },
        "id": "9F1FcrTos0x4",
        "outputId": "19be9ef6-b0dc-4374-c0d6-26a2c4b6df73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+-------+\n",
            "|  col_name|data_type|comment|\n",
            "+----------+---------+-------+\n",
            "|   OrderID|      int|   NULL|\n",
            "| OrderDate|     date|   NULL|\n",
            "|CustomerID|   string|   NULL|\n",
            "|   Product|   string|   NULL|\n",
            "|  Quantity|      int|   NULL|\n",
            "|     Price|   double|   NULL|\n",
            "+----------+---------+-------+\n",
            "\n",
            "+----------+---------+-------+\n",
            "|  col_name|data_type|comment|\n",
            "+----------+---------+-------+\n",
            "|   OrderID|      int|   NULL|\n",
            "| OrderDate|   string|   NULL|\n",
            "|CustomerID|   string|   NULL|\n",
            "|   Product|   string|   NULL|\n",
            "|  Quantity|      int|   NULL|\n",
            "|     Price|   double|   NULL|\n",
            "+----------+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. Building a Reliable Data Lake with Delta Lake:\n",
        "# Implement schema enforcement and handle data updates with Delta Lake.\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"OrderID\", IntegerType(), False),\n",
        "    StructField(\"OrderDate\", StringType(), False),\n",
        "    StructField(\"CustomerID\", StringType(), False),\n",
        "    StructField(\"Product\", StringType(), False),\n",
        "    StructField(\"Quantity\", IntegerType(), False),\n",
        "    StructField(\"Price\", DoubleType(), False)\n",
        "])\n",
        "\n",
        "new_sales_df = spark.read \\\n",
        "    .schema(schema) \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"dbfs:/FileStore/new_sales_data.csv\")\n",
        "\n",
        "new_sales_df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"dbfs:/FileStore/delta/new_sales_delta_table\")\n",
        "\n",
        "# Perform the MERGE INTO operation\n",
        "spark.sql(\"\"\"\n",
        "MERGE INTO delta.`dbfs:/FileStore/delta/sales_delta_table` AS target\n",
        "USING delta.`dbfs:/FileStore/delta/new_sales_delta_table` AS source\n",
        "ON target.OrderID = source.OrderID\n",
        "WHEN MATCHED THEN\n",
        "  UPDATE SET\n",
        "    target.OrderDate = source.OrderDate,\n",
        "    target.CustomerID = source.CustomerID,\n",
        "    target.Product = source.Product,\n",
        "    target.Quantity = source.Quantity,\n",
        "    target.Price = source.Price\n",
        "WHEN NOT MATCHED THEN\n",
        "  INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price)\n",
        "  VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(\"DESCRIBE delta.`dbfs:/FileStore/delta/sales_delta_table`\").show()\n",
        "spark.sql(\"DESCRIBE delta.`dbfs:/FileStore/delta/new_sales_delta_table`\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5846c78e-9800-45a9-874d-2b4451ebf6ed",
          "showTitle": false,
          "title": ""
        },
        "id": "WYx4IzmTs0x4",
        "outputId": "99e9dceb-92ad-49f8-8926-704ac5a4bed3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[path: string]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Optimize data layout and perform vacuum operations to maintain storage efficiency.\n",
        "spark.sql(\"OPTIMIZE delta.`dbfs:/FileStore/delta/sales_delta_table`ZORDER BY (OrderDate)\")\n",
        "spark.sql(\"VACUUM delta.`dbfs:/FileStore/delta/sales_delta_table` RETAIN 168 HOURS\")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Assignment 4",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
